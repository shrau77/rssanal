import os
import json
import uuid
import time
import logging
import re
import random
from io import BytesIO
from datetime import datetime

import feedparser
import requests
from bs4 import BeautifulSoup
from PIL import Image
import pytesseract
from huggingface_hub import InferenceClient
from github import Github
from curl_cffi import requests as cf_requests

# --- CONFIG ---
HF_TOKEN = os.environ.get("HF_TOKEN")
GITHUB_TOKEN = os.environ.get("MY_GITHUB_TOKEN")
REPO_NAME = os.environ.get("GITHUB_REPOSITORY")
MODEL_ID = "Qwen/Qwen2.5-72B-Instruct" 
RSS_URL = "https://ntc.party/posts.rss"

# –°–ø–∏—Å–∫–∏ –∂–∏–≤—ã—Ö –ø—Ä–æ–∫—Å–∏ (HTTP/HTTPS)
PROXY_SOURCES = [
    "https://raw.githubusercontent.com/TheSpeedX/SOCKS-List/master/http.txt",
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/http.txt",
    "https://raw.githubusercontent.com/zloi-user/hideip.me/main/http.txt"
]

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

KEYWORDS = [
    "vless", "vmess", "trojan", "fragment", "mtu", "noise", "packet-len", 
    "mtn", "beeline", "megafon", "mts", "tele2", "yota", "rostelecom", 
    "shadowsocks", "pbkdf2", "argon2", "hysteria", "amnezia", "xray", "sing-box",
    "reality", "grpc", "ws", "tcp", "warp", "wireguard"
]

class ProxyManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –∂–∏–≤–æ–≥–æ –ø—Ä–æ–∫—Å–∏"""
    def __init__(self):
        self.proxies = []

    def fetch_proxies(self):
        logger.info("Fetching fresh proxies...")
        for source in PROXY_SOURCES:
            try:
                r = requests.get(source, timeout=10)
                if r.status_code == 200:
                    lines = r.text.strip().split('\n')
                    logger.info(f"Loaded {len(lines)} proxies from {source}")
                    self.proxies.extend(lines)
            except Exception as e:
                logger.error(f"Error fetching proxy list: {e}")
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ
        random.shuffle(self.proxies)
        self.proxies = list(set(self.proxies)) # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏
        logger.info(f"Total unique proxies to try: {len(self.proxies)}")

    def get_working_session(self, test_url):
        """–ü–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–∫—Å–∏, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ—Ç —Ä–∞–±–æ—á–∏–π –¥–ª—è curl_cffi"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏ (–≤–¥—Ä—É–≥ –ø–æ–≤–µ–∑–µ—Ç?)
        try:
            logger.info("Trying direct connection...")
            sess = cf_requests.Session(impersonate="chrome120")
            resp = sess.get(test_url, timeout=10)
            if resp.status_code == 200:
                logger.info("Direct connection worked!")
                return sess
        except Exception:
            logger.info("Direct connection failed. Starting Proxy Roulette...")

        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ–∫—Å–∏
        # –û–≥—Ä–∞–Ω–∏—á–∏–º –ø–æ–ø—ã—Ç–∫–∏, —á—Ç–æ–±—ã –Ω–µ –≤–∏—Å–µ—Ç—å –≤–µ—á–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 20 –ø–æ–ø—ã—Ç–æ–∫)
        max_tries = 30
        for i, proxy_addr in enumerate(self.proxies[:max_tries]):
            proxy_url = f"http://{proxy_addr.strip()}"
            logger.info(f"[{i+1}/{max_tries}] Testing proxy: {proxy_url}")
            
            try:
                sess = cf_requests.Session(impersonate="chrome120")
                sess.proxies = {"http": proxy_url, "https": proxy_url}
                
                # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                resp = sess.get(test_url, timeout=15)
                
                if resp.status_code == 200:
                    logger.info(f"üéâ SUCCESS! Found working proxy: {proxy_url}")
                    return sess
                else:
                    logger.warning(f"Proxy returned status {resp.status_code}")
            
            except Exception as e:
                # –û—à–∏–±–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º, –ø—Ä–æ—Å—Ç–æ –∏–¥–µ–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É
                pass
        
        raise Exception("All proxies failed. Cloudflare won today.")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å–µ—Å—Å–∏—è
proxy_manager = ProxyManager()
proxy_manager.fetch_proxies()
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ—Å—Å–∏—é –æ–¥–∏–Ω —Ä–∞–∑
global_session = proxy_manager.get_working_session(RSS_URL)

class OCRProcessor:
    @staticmethod
    def extract_text_from_image_url(url):
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ —Å–µ—Å—Å–∏—é (—Ç–æ—Ç –∂–µ –ø—Ä–æ–∫—Å–∏) –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
            response = global_session.get(url, timeout=20)
            if response and response.status_code == 200:
                img = Image.open(BytesIO(response.content))
                text = pytesseract.image_to_string(img, lang='rus+eng')
                return text
        except Exception as e:
            logger.error(f"OCR Error: {e}")
        return ""

class AIAnalyst:
    def __init__(self):
        self.client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)

    def analyze(self, text):
        prompt = f"""
–¢—ã –ø–∞—Ä—Å–µ—Ä —Ñ–æ—Ä—É–º–∞. –ò–∑–≤–ª–µ–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–æ–≤ VPN.
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON.

–ü—Ä–∞–≤–∏–ª–∞:
1. "type": "CONFIG" (–ø—Ä–æ—Ç–æ–∫–æ–ª—ã), "COSMETICS" (–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–∏–ø–∞ mtu), "FULL" (–æ–±–∞), "EXTERNAL" (—Å—Å—ã–ª–∫–∏), "GARBAGE".
2. "region"/"provider": –û–ø—Ä–µ–¥–µ–ª–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (MTS, Beeline, Rostelecom, Moscow, SPb). –¢—Ä–∞–Ω—Å–ª–∏—Ç. –ï—Å–ª–∏ –Ω–µ—Ç - null.
3. "config": –ü–æ–ª–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ JSON.
4. "cosmetics": –ü–æ–ª—è fragment, mtu, noise, split-http.
5. "summary": –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

Schema:
{{
  "type": "string",
  "region": "string or null",
  "provider": "string or null",
  "config": "string or null",
  "cosmetics": {{ "fragment": "string", "mtu": "int", "noise": "string" }},
  "summary": "string",
  "source_url": "string"
}}

–¢–µ–∫—Å—Ç:
{text[:4000]} 
""" 
        try:
            response = self.client.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.1
            )
            content = response.choices[0].message.content
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            return None
        except Exception as e:
            logger.error(f"AI Error: {e}")
            return None

class GitHubManager:
    def __init__(self):
        self.gh = Github(GITHUB_TOKEN)
        self.repo = self.gh.get_repo(REPO_NAME)

    def save_data(self, ai_data, meta_data):
        try:
            region = ai_data.get('region') or "Unknown"
            provider = ai_data.get('provider') or "Unknown"
            
            clean_reg = "".join([c for c in region if c.isalnum() or c in (' ', '-', '_')]).strip()
            clean_prov = "".join([c for c in provider if c.isalnum() or c in (' ', '-', '_')]).strip()
            
            filename = f"{int(time.time())}_{str(uuid.uuid4())[:6]}.json"
            path = f"configs/{clean_reg}/{clean_prov}/{filename}"

            final_json = { "meta": meta_data, "data": ai_data }
            content = json.dumps(final_json, ensure_ascii=False, indent=2)
            
            self.repo.create_file(path=path, message=f"Add: {clean_prov}", content=content)
            logger.info(f"GitHub Saved: {path}")
            return True
        except Exception as e:
            logger.error(f"GitHub Save Error: {e}")
            return False

def main():
    logger.info("--- Starting Scraper (Proxy Mode) ---")
    
    # 1. RSS —á–µ—Ä–µ–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø—Ä–æ–∫—Å–∏
    try:
        resp = global_session.get(RSS_URL, timeout=30)
        feed = feedparser.parse(resp.content)
        logger.info(f"Entries found: {len(feed.entries)}")
    except Exception as e:
        logger.error(f"Fatal: Could not fetch RSS even with proxies. {e}")
        return

    for entry in feed.entries[:15]: 
        try:
            guid = entry.get('id', entry.get('link'))
            logger.info(f"Processing: {entry.title}")

            soup = BeautifulSoup(entry.description, 'lxml')
            text_content = soup.get_text(separator="\n")
            
            ocr_text = ""
            for img in soup.find_all('img'):
                src = img.get('src')
                if src:
                    if src.startswith('/'): src = "https://ntc.party" + src
                    if "emoji" not in src:
                        ocr_text += OCRProcessor.extract_text_from_image_url(src) + "\n"

            full_text = f"{entry.title}\n{text_content}\nOCR:\n{ocr_text}"

            if not any(k in full_text.lower() for k in KEYWORDS):
                continue

            analyst = AIAnalyst()
            result = analyst.analyze(full_text)
            
            if result and result.get('type') != 'GARBAGE':
                result['source_url'] = entry.link
                gh = GitHubManager()
                meta = { "guid": guid, "date": datetime.now().isoformat(), "host": "GH Actions + Proxy" }
                gh.save_data(result, meta)
                
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"Entry Error: {e}")

if __name__ == "__main__":
    main()
